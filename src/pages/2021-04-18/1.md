---
title: Promise成本及有限并发

# []: (new Date()).toJSON()
date: '2021-04-18T13:40:25.529Z'
---

## 缘由

最近在关注面试相关的东西，知乎上偶然就遇到这么一则帖子：[JavaScript 面试题一则：有限并行](https://zhuanlan.zhihu.com/p/360193435)。简单概括，面试官出题询问`await Promise.all(urls.map(url => fetch(url)))`与`const results = [];for (const url of urls ) {results.push(await fetch(urls[i]))}`两者的区别与优劣，包括是否有优化的地方。借由串行不够高效、全并发瞬时创建大量的 Promise 和函数上下文影响性能为由，引出了有限并发的意义及实现。这个面试题也让我联想到了曾经使用[Taro](https://github.com/NervJS/taro)时框架开箱提供对`wx.request`的[并发控制](https://developers.weixin.qq.com/miniprogram/dev/framework/ability/network.html#使用限制)。

## 一点点吐槽：大量创建 Promise 消耗有多大？

现代三大前端框架均使用了虚拟 DOM 的形式来管理渲染树，假设创建对象与函数上下文的消耗大到需要做并发控制，那么虚拟 DOM 优化方案本身就毫无立足之本：每个组件都（至少）是一个函数，都会产出一个新的组件对象，页面的渲染本身就会成为不可接受的性能成本；同时 Promise 作为一个原型就是 native code 的构造器，原型链负担可比许多传统对象都要低，加上作为新生宠儿，两个优势理应都给优化带来极大的操作空间。
为验证观点，这里采用在空白页面的 console 中运行以下三句代码并利用 Performance 面板来记录的运行 cpu 及内存消耗的方法进行比对，表格中括号值为与第一行（创建大数组操作）的比值。

```js
urls = new Array(10 ** 6).fill('')
random = urls.map(() => `${Math.random()}`)
promise = await random.map((s) => new Promise((r) => r(`1${s}`)))
```

| 行号 | 运行时间(ms)    | 运行前    | 运行后    | 增值              |
| ---- | --------------- | --------- | --------- | ----------------- |
| 1    | 364.81          | 90863712  | 114394488 | 23530776          |
| 2    | 509.05(139.54%) | 114394488 | 175121296 | 60726808(258.07%) |
| 3    | 425.70(116.69%) | 175121296 | 243238820 | 68117524(289.48%) |

通过这个表格我们可以注意到，比起创建一个同等长度的随机数字符串数组，创建同等长度的 Promise 对象数组更快，内存占用虽稍高但也仅仅高出 12.17%；同时比起创建大量对象，创建一个大数组本身的消耗更加可观。也就是，这个优化的前提是并不成立的，或者说**不能单依靠这个前提就推导出有限并发的存在必要**。

> 是否可以这么想：与其探究并发控制，不如研究为何出现大并发本身？毕竟就算控制了并发量，你也无法避开创建大数组来存放 fetch 结果，加上一般数据的使用前还得进行数据处理等，这个创建可能要出现数次。

> 以上说法并不能代表你在实际项目中就可以数十数百地并行 fetch。一来浏览器本身就有并发控制，我们对同一个域名的请求一般只有 6 到 8 个（具体看浏览器实现）；二来创建 http 请求本身消耗并不像创建 Promise 这样低（websocket 是否会好点？）。
